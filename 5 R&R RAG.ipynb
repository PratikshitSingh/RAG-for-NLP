{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "**Use Case Title:**  \n",
    "**\"Research Paper Selector using Retrieve-and-Rerank RAG (R&R-RAG)\"**\n",
    "\n",
    "**Problem Statement:**  \n",
    "In academic research, retrieving the most relevant scholarly papers for a specific topic (e.g., \"few-shot learning techniques\") can be challenging due to the large volume of documents and noisy keyword-based results. A basic semantic search using embeddings is often insufficient in terms of ranking the best-matching results based on fine-grained semantic nuances.\n",
    "\n",
    "To improve the **accuracy and relevance** of retrieved results, this project implements a **hybrid RAG pipeline** using:\n",
    "- A **bi-encoder (SentenceTransformer)** for fast semantic retrieval using FAISS.\n",
    "- A **cross-encoder (MS MARCO TinyBERT)** for reranking the top retrieved results using deeper interaction modeling between query and document.\n",
    "\n",
    "This approach enhances **information retrieval quality** for NLP/NLU-based academic literature search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's new?\n",
    "# CSV-based structured document handling\n",
    "# CrossEncoder reranking added\n",
    "# Uses CrossEncoder for deep reranking\n",
    "# Focus is on ranking, not answering\n",
    "# Reranked paper results with relevance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Significance\n",
    "\n",
    "This pattern (R&R-RAG) is used in:\n",
    "- **Academic search engines** like Semantic Scholar, Arxiv-Sanity.\n",
    "- **Legal document analysis** for ranking contract clauses by importance.\n",
    "- **Patent retrieval** and **systematic literature reviews** in NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install sentence-transformers faiss-cpu pandas tqdm -q\n",
    "# tqdm - For showing progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 10 documents.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load academic papers dataset\n",
    "# Example format: papers.csv with 'title' and 'abstract'\n",
    "df = pd.read_csv(\"papers.csv\")  # <- Replace with your corpus\n",
    "df['content'] = df['title'] + \". \" + df['abstract']\n",
    "documents = df['content'].tolist()\n",
    "print(f\"âœ… Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: Few-Shot Learning via Prompt Tuning with LLMs. We propose a prompt-tuning strategy using large language models for adapting to few-shot settings in NLP tasks. Our method reduces the need for fine-tuning by leveraging prompt engineering.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample document:\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Few-Shot Learning via Prompt Tuning with LLMs. We propose a prompt-tuning strategy using large language models for adapting to few-shot settings in NLP tasks. Our method reduces the need for fine-tuning by leveraging prompt engineering.',\n",
       " 'Meta-Learning for Efficient Few-Shot Classification. Meta-learning frameworks have shown promising results in few-shot classification by optimizing the initialization of neural networks across tasks.',\n",
       " 'A Survey on Transformers in Vision. This paper surveys the use of Transformer architectures in computer vision, including ViT, DETR, and Swin Transformers, with benchmarks and comparisons.',\n",
       " 'Contrastive Learning for Representation Learning. We explore contrastive learning approaches that learn useful representations by pulling semantically similar instances together and pushing dissimilar ones apart.',\n",
       " 'Neural Scaling Laws in Large Language Models. This work investigates how performance scales with model size, dataset size, and compute, providing insights into neural scaling laws.',\n",
       " 'Efficient Retrieval Techniques for Long Documents. We introduce retrieval mechanisms to efficiently index and access long documents in QA systems, enabling fast recall without sacrificing semantic coverage.',\n",
       " 'Reinforcement Learning with Human Feedback. We discuss reinforcement learning with human feedback (RLHF), which allows agents to align better with human values through reward modeling.',\n",
       " 'An Empirical Study of LLM Prompt Sensitivity. This empirical study examines how LLMs respond to various prompt formulations across tasks like summarization, translation, and classification.',\n",
       " 'Self-Supervised Pretraining in NLP. This paper analyzes self-supervised learning objectives for language modeling and their transferability to downstream tasks.',\n",
       " 'Benchmarks for Multimodal Understanding. We present a collection of benchmarks and evaluation frameworks for assessing multimodal understanding across image-text tasks.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e9f42dcf944e468ce3e51e7ce0fcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Embed documents using SentenceTransformer (bi-encoder)\n",
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "doc_embeddings = bi_encoder.encode(documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create FAISS index\n",
    "dimension = doc_embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "print(\"âœ… FAISS index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4f7d50a3934531b73fbdbca19c9cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ebf7060b5e41ea9691400a6e685d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e4338b66a141219b04e4ac19cd52a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a3e7e8dcf8418483943f86e34870b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746fd69d6a27442e966fb48f671f5d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98732a64a45243cfa0ec24d87f92919e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Cross-Encoder for reranking (query-doc pairs)\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\", max_length=512)\n",
    "# Loads a cross-encoder model (TinyBERT) that is trained on MS MARCO â€” a benchmark for question-answer relevance.\n",
    "# The model takes pairs of (query, document) and predicts a similarity score.\n",
    "# max_length=512 ensures the combined input length is trimmed or padded appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Retrieval + Reranking pipeline\n",
    "# Defines a function that takes a userâ€™s question and returns the top-k best-matched academic papers.\n",
    "def retrieve_and_rerank(query, top_k=10):\n",
    "    # Step 1: Vector search (fast recall)\n",
    "    query_embedding = bi_encoder.encode([query]) # Embeds the user's query into a dense vector using the bi-encoder.\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k * 5) # Searches the FAISS index to get the top 50 most similar documents based on vector similarity (5Ã— top_k for broader initial recall).\n",
    "    initial_results = [(documents[i], df.iloc[i]['title'], df.iloc[i]['abstract']) for i in indices[0]]\n",
    "\n",
    "    # Step 2: Cross-encoder reranking\n",
    "    rerank_pairs = [[query, doc] for doc, _, _ in initial_results] # Creates a list of pairs: [query, doc] â€” required input format for cross-encoder prediction.\n",
    "    scores = cross_encoder.predict(rerank_pairs) # Gets back relevance scores (e.g., 0.9 = very relevant, 0.1 = not relevant).\n",
    "\n",
    "    # Combines scores with initial results using zip()\n",
    "    # Sorts them in descending order of score.\n",
    "    # Returns the top_k (e.g., top 10 papers).\n",
    "    reranked = sorted(zip(scores, initial_results), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return reranked[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top results for: What are the latest techniques in few-shot learning?\n",
      "\n",
      "ðŸ“ Title: Few-Shot Learning via Prompt Tuning with LLMs\n",
      "ðŸ“Š Score: 0.9026\n",
      "ðŸ“„ Abstract: We propose a prompt-tuning strategy using large language models for adapting to few-shot settings in NLP tasks. Our method reduces the need for fine-tuning by leveraging prompt engineering....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Meta-Learning for Efficient Few-Shot Classification\n",
      "ðŸ“Š Score: 0.7122\n",
      "ðŸ“„ Abstract: Meta-learning frameworks have shown promising results in few-shot classification by optimizing the initialization of neural networks across tasks....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Contrastive Learning for Representation Learning\n",
      "ðŸ“Š Score: 0.0003\n",
      "ðŸ“„ Abstract: We explore contrastive learning approaches that learn useful representations by pulling semantically similar instances together and pushing dissimilar ones apart....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Reinforcement Learning with Human Feedback\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: We discuss reinforcement learning with human feedback (RLHF), which allows agents to align better with human values through reward modeling....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Self-Supervised Pretraining in NLP\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: This paper analyzes self-supervised learning objectives for language modeling and their transferability to downstream tasks....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Efficient Retrieval Techniques for Long Documents\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: We introduce retrieval mechanisms to efficiently index and access long documents in QA systems, enabling fast recall without sacrificing semantic coverage....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: An Empirical Study of LLM Prompt Sensitivity\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: This empirical study examines how LLMs respond to various prompt formulations across tasks like summarization, translation, and classification....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: A Survey on Transformers in Vision\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: This paper surveys the use of Transformer architectures in computer vision, including ViT, DETR, and Swin Transformers, with benchmarks and comparisons....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Neural Scaling Laws in Large Language Models\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: This work investigates how performance scales with model size, dataset size, and compute, providing insights into neural scaling laws....\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ“ Title: Benchmarks for Multimodal Understanding\n",
      "ðŸ“Š Score: 0.0002\n",
      "ðŸ“„ Abstract: We present a collection of benchmarks and evaluation frameworks for assessing multimodal understanding across image-text tasks....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "query = \"What are the latest techniques in few-shot learning?\"\n",
    "results = retrieve_and_rerank(query)\n",
    "\n",
    "print(f\"\\nðŸ” Top results for: {query}\\n\")\n",
    "for score, (doc, title, abstract) in results:\n",
    "    print(f\"ðŸ“ Title: {title}\")\n",
    "    print(f\"ðŸ“Š Score: {score:.4f}\")\n",
    "    print(f\"ðŸ“„ Abstract: {abstract[:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
