{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Organizations often need to analyze **large volumes of legal contracts** stored as PDFs, which contain nuanced clauses like **termination**, **non-compete**, and **confidentiality terms**. Purely semantic search (vector-based) or keyword search (BM25) alone may miss context or relevance.\n",
    "\n",
    "**Objective:**  \n",
    "To build a more powerful **Hybrid RAG system** over multiple legal contracts that combines:\n",
    "\n",
    "- **Semantic search** via FAISS for meaning-based retrieval  \n",
    "- **Keyword search** via BM25 (Whoosh) for exact match and legal precision  \n",
    "- **Instruction-tuned generation** using FLAN-T5 to answer questions using merged results  \n",
    "\n",
    "All achieved **without cloud APIs**, using open-source tools only.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats new?\n",
    "# Focused on legal contracts\n",
    "# Hybrid retrieval (semantic + BM25 keyword search)\n",
    "# Whoosh (BM25-based inverted index)\n",
    "# Combines FAISS + Whoosh, removes duplicates\n",
    "# Stores file-level metadata with chunks\n",
    "# Allows summarization of each PDF in a legal context\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install ipywidgets sentence-transformers faiss-cpu transformers PyPDF2 whoosh -q\n",
    "# whoosh - For full-text keyword search (BM25-style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "# Imports schema field types used to define the structure of a Whoosh keyword search index.\n",
    "# TEXT: For full-text searchable fields\n",
    "# ID: For non-tokenized IDs (e.g., file name)\n",
    "# i.e. Defines how chunks and metadata (like file name) are stored for BM25-style search.\n",
    "from whoosh.index import create_in\n",
    "# Used to create a Whoosh index directory and store your keyword-searchable documents in it.\n",
    "from whoosh.qparser import QueryParser\n",
    "# lets you parse a natural-language query string into a Whoosh search query object.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 466 chunks from contracts.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load & chunk PDFs from contracts/\n",
    "def load_contract_chunks(folder_path, chunk_size=300):\n",
    "    chunks = []\n",
    "    filenames = os.listdir(folder_path)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "            reader = PyPDF2.PdfReader(path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                chunk = text[i:i + chunk_size]\n",
    "                chunks.append((chunk, filename))\n",
    "    return chunks\n",
    "\n",
    "chunks_with_meta = load_contract_chunks(\"contracts\", chunk_size=300)\n",
    "chunks = [c[0] for c in chunks_with_meta] # Extracts just the chunk texts (without filenames) for FAISS indexing.\n",
    "print(f\"✅ Loaded {len(chunks)} chunks from contracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks_with_meta: [('  Page 1 Sample Contract    Contract No.___________  PROFESSIONAL SERVICES AGREEMENT      THIS AGREEMENT made and entered into this _______day of                       , 20      by and between the SANTA  CRUZ COUNTY REGIONAL TRANSPORTATION COMMISSION, hereinafter called COMMISSION, and ________     ', '1SampleCo1ntract-Shuttle.pdf')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"chunks_with_meta: {chunks_with_meta[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks ['  Page 1 Sample Contract    Contract No.___________  PROFESSIONAL SERVICES AGREEMENT      THIS AGREEMENT made and entered into this _______day of                       , 20      by and between the SANTA  CRUZ COUNTY REGIONAL TRANSPORTATION COMMISSION, hereinafter called COMMISSION, and ________     ']\n"
     ]
    }
   ],
   "source": [
    "print(f\"chunks {chunks[:1]}\") # Print first chunk for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 384\n",
      "✅ FAISS index ready.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: FAISS vector index (semantic search)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Loads the pretrained bi-encoder model all-MiniLM-L6-v2 from HuggingFace via SentenceTransformers.\n",
    "embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
    "dimension = len(embeddings[0])\n",
    "print(f\"Dimension: {dimension}\")\n",
    "faiss_index = faiss.IndexFlatL2(dimension) # Creates a flat L2 index in FAISS for efficient nearest-neighbor search using Euclidean distance (L2 norm).\n",
    "faiss_index.add(np.array(embeddings)) # Adds all your chunk embeddings into the FAISS index.\n",
    "print(\"✅ FAISS index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings: [[-0.02682767  0.07159669  0.00220114 ...  0.0158756  -0.02813633\n",
      "  -0.05008272]\n",
      " [-0.03836435  0.00306136 -0.06537561 ...  0.08085535 -0.02805743\n",
      "  -0.03017664]\n",
      " [-0.04938861 -0.02651223 -0.09722698 ... -0.00675625  0.01230283\n",
      "  -0.06103821]\n",
      " ...\n",
      " [-0.0593512   0.08018056  0.02705275 ... -0.00472621  0.09366456\n",
      "   0.03406013]\n",
      " [ 0.01756999  0.07844537 -0.01589446 ... -0.06351048  0.04946419\n",
      "  -0.03323623]\n",
      " [-0.07469863  0.11128418  0.00327795 ... -0.10253063  0.04803063\n",
      "  -0.0106685 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searchable faiss_index: <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x16898b690> >\n"
     ]
    }
   ],
   "source": [
    "print(f\"searchable faiss_index: {faiss_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whoosh BM25 index ready.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: BM25 index using Whoosh (keyword search)\n",
    "# This step builds a BM25-based full-text search index using Whoosh — the keyword retrieval engine of your Hybrid RAG pipeline.\n",
    "\n",
    "schema = Schema(content=TEXT(stored=True), path=ID(stored=True))\n",
    "# Defines a schema for the Whoosh index with two fields:\n",
    "# content: The actual text chunk (full-text searchable using BM25)\n",
    "# path: The source filename (used as metadata)\n",
    "\n",
    "index_dir = tempfile.mkdtemp()\n",
    "ix = create_in(index_dir, schema) # Initializes a new Whoosh index in the temporary folder with your schema.\n",
    "writer = ix.writer()\n",
    "for i, (chunk, fname) in enumerate(chunks_with_meta):\n",
    "    writer.add_document(content=chunk, path=fname)\n",
    "writer.commit() # Commits the changes to the index, making it searchable.\n",
    "print(\"✅ Whoosh BM25 index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Hybrid retrieval\n",
    "def hybrid_retrieve(query, top_k=3):\n",
    "    # 1. Vector search (semantic)\n",
    "    # If the user says \"What is the notice period?\", FAISS will find chunks that semantically align \n",
    "    # even if the words are not an exact match.\n",
    "    q_vec = embedder.encode([query]) # Converts the query to a dense vector.\n",
    "    _, indices = faiss_index.search(np.array(q_vec), top_k) # Retrieves top-k closest chunk embeddings.\n",
    "    semantic_results = [chunks[i] for i in indices[0]] # Contains text chunks most semantically similar to the question\n",
    "    \n",
    "    # 2. Keyword search (BM25)\n",
    "    # FAISS might miss exact matches, while keyword search shines when the query and answer have overlapping \n",
    "    # tokens (e.g., legal clauses, names, technical terms).\n",
    "    with ix.searcher() as searcher:\n",
    "        parser = QueryParser(\"content\", schema=ix.schema)\n",
    "        parsed_query = parser.parse(query)\n",
    "        results = searcher.search(parsed_query, limit=top_k)\n",
    "        keyword_results = [r['content'] for r in results]\n",
    "\n",
    "    # Merge and dedupe\n",
    "    # Combines semantic and keyword results\n",
    "    hybrid_results = list(dict.fromkeys(semantic_results + keyword_results))\n",
    "    return hybrid_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8a0b5104454ecbb3e3757e3bf6e9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0993668284fe434d95cd8ecf331eae9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d94e45f0d643d5838dd17646e5a2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f186c7d1a34f7fb46dfda77f8ceb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a76c92c77d45f483f2ffee16c4f91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a68faeb9774e4d92d6f92ff15799dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891cff09fdaf4ef0b2cf953ccacf7433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Step 5: LLM - FLAN-T5\n",
    "qa = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Answer query with hybrid context\n",
    "def answer_query(query):\n",
    "    contexts = hybrid_retrieve(query) # Calls the previous hybrid retriever to get top-k relevant chunks.\n",
    "    full_context = \"\\n\".join(contexts) # Combines the chunks into a single paragraph for the prompt.\n",
    "    prompt = f\"Context:\\n{full_context}\\n\\nQuestion: {query}\\n\\nAnswer:\" # Constructs an instruction-style prompt with context + question.\n",
    "    result = qa(prompt)[0][\"generated_text\"]\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Query 1:\n",
      "i) Breac\n",
      "\n",
      "🧠 Query 2:\n",
      "An agreement of service through which an employee commits not to compete with his employer is not in restraint of trad perform his obligations under a contr act. F) Discharge by impossibility of performance – Impossibility of performance results in the discharge of the contract. An agreement which is impossible is void, because law does not comp or other forms of compensation; a nd selection for training (including apprenticeship), employment, upgrading, demotion, or transfer. The CONSULTANT agrees to post in conspicuous places, available to employees and applicants for employme nt, notice setting forth the provisions of this non-discrim\n",
      "\n",
      "🧠 Query 3:\n",
      "a docto r has a duty of confidentiality oses a special duty to act with the utmost good faith i.e., to disclose all material information\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "print(\"\\n🧠 Query 1:\")\n",
    "print(answer_query(\"What does the termination clause say?\"))\n",
    "\n",
    "print(\"\\n🧠 Query 2:\")\n",
    "print(answer_query(\"Explain the non-compete obligations.\"))\n",
    "\n",
    "print(\"\\n🧠 Query 3:\")\n",
    "print(answer_query(\"Describe the confidentiality terms.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Query 4:\n",
      "a) a written progress report, in a format to be mutually agreed upon, that is sufficiently detailed for the Contract Manager to determ ine if the CONSULTANT is performing to expectations and is on sche dule; 6. Written progress reports, in a format to be mutually agreed upon, that is sufficiently detailed for the Contract Manager to determ ine if the CONSULTANT is performing to expectations and is on sche dule; provides communi\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧠 Query 4:\")\n",
    "print(answer_query(\"Summarize each PDF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
